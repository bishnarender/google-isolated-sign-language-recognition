{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a14bca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:07:13.728826Z",
     "iopub.status.busy": "2023-06-02T01:07:13.727671Z",
     "iopub.status.idle": "2023-06-02T01:07:21.777025Z",
     "shell.execute_reply": "2023-06-02T01:07:21.776043Z"
    },
    "papermill": {
     "duration": 8.058286,
     "end_time": "2023-06-02T01:07:21.779522",
     "exception": false,
     "start_time": "2023-06-02T01:07:13.721236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89363d22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:07:21.790998Z",
     "iopub.status.busy": "2023-06-02T01:07:21.790331Z",
     "iopub.status.idle": "2023-06-02T01:07:21.798657Z",
     "shell.execute_reply": "2023-06-02T01:07:21.797676Z"
    },
    "papermill": {
     "duration": 0.016714,
     "end_time": "2023-06-02T01:07:21.801215",
     "exception": false,
     "start_time": "2023-06-02T01:07:21.784501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8773750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:07:21.812797Z",
     "iopub.status.busy": "2023-06-02T01:07:21.811959Z",
     "iopub.status.idle": "2023-06-02T01:07:21.818669Z",
     "shell.execute_reply": "2023-06-02T01:07:21.817813Z"
    },
    "papermill": {
     "duration": 0.01459,
     "end_time": "2023-06-02T01:07:21.820667",
     "exception": false,
     "start_time": "2023-06-02T01:07:21.806077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    \"\"\"Read a JSON file and parse it into a Python object.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file to read.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary object representing the JSON data.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file path does not exist.\n",
    "        ValueError: If the specified file path does not contain valid JSON data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the file and load the JSON data into a Python object\n",
    "        with open(file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "        return json_data\n",
    "    except FileNotFoundError:\n",
    "        # Raise an error if the file path does not exist\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    except ValueError:\n",
    "        # Raise an error if the file does not contain valid JSON data\n",
    "        raise ValueError(f\"Invalid JSON data in file: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b59bf0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:07:21.832160Z",
     "iopub.status.busy": "2023-06-02T01:07:21.831375Z",
     "iopub.status.idle": "2023-06-02T01:07:22.143091Z",
     "shell.execute_reply": "2023-06-02T01:07:22.141664Z"
    },
    "papermill": {
     "duration": 0.319818,
     "end_time": "2023-06-02T01:07:22.145410",
     "exception": false,
     "start_time": "2023-06-02T01:07:21.825592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "... LOAD SIGN TO PREDICTION INDEX MAP FROM JSON FILE ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train_df = pd.read_csv('train.csv')\n",
    "# s2p_map = {k.lower():v for k,v in read_json_file(os.path.join(\"sign_to_prediction_index_map.json\")).items()}\n",
    "# p2s_map = {v:k for k,v in read_json_file(os.path.join(\"sign_to_prediction_index_map.json\")).items()}\n",
    "\n",
    "train_df = pd.read_csv('/kaggle/input/asl-signs/train.csv')\n",
    "print(\"\\n\\n... LOAD SIGN TO PREDICTION INDEX MAP FROM JSON FILE ...\\n\")\n",
    "\n",
    "s2p_map = {k.lower():v for k,v in read_json_file(os.path.join(\"/kaggle/input/asl-signs/sign_to_prediction_index_map.json\")).items()}\n",
    "# {k: s2p_map[k] for k in list(s2p_map)[:2]} => {'tv': 0, 'after': 1}\n",
    "p2s_map = {v:k for k,v in read_json_file(os.path.join(\"/kaggle/input/asl-signs/sign_to_prediction_index_map.json\")).items()}\n",
    "# {k: p2s_map[k] for k in list(p2s_map)[:2]} => {0: 'TV', 1: 'after'}\n",
    "encoder = lambda x: s2p_map.get(x.lower())\n",
    "decoder = lambda x: p2s_map.get(x)\n",
    "# print(s2p_map)\n",
    "train_df['label'] = train_df.sign.map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ace1e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:07:22.157512Z",
     "iopub.status.busy": "2023-06-02T01:07:22.157188Z",
     "iopub.status.idle": "2023-06-02T01:07:22.183147Z",
     "shell.execute_reply": "2023-06-02T01:07:22.182123Z"
    },
    "papermill": {
     "duration": 0.035863,
     "end_time": "2023-06-02T01:07:22.186426",
     "exception": false,
     "start_time": "2023-06-02T01:07:22.150563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "708\n"
     ]
    }
   ],
   "source": [
    "ROWS_PER_FRAME = 543\n",
    "MAX_LEN = 384\n",
    "CROP_LEN = MAX_LEN\n",
    "NUM_CLASSES  = 250\n",
    "PAD = -100.\n",
    "\n",
    "NOSE=[\n",
    "    1,2,98,327\n",
    "]\n",
    "LNOSE = [98]\n",
    "RNOSE = [327]\n",
    "LIP = [ 0, \n",
    "    61, 185, 40, 39, 37, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "LLIP = [84,181,91,146,61,185,40,39,37,87,178,88,95,78,191,80,81,82]\n",
    "RLIP = [314,405,321,375,291,409,270,269,267,317,402,318,324,308,415,310,311,312]\n",
    "\n",
    "POSE = [500, 502, 504, 501, 503, 505, 512, 513]\n",
    "LPOSE = [513,505,503,501]\n",
    "RPOSE = [512,504,502,500]\n",
    "\n",
    "REYE = [\n",
    "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "    246, 161, 160, 159, 158, 157, 173,\n",
    "]\n",
    "LEYE = [\n",
    "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
    "    466, 388, 387, 386, 385, 384, 398,\n",
    "]\n",
    "\n",
    "LHAND = np.arange(468, 489).tolist()\n",
    "RHAND = np.arange(522, 543).tolist()\n",
    "\n",
    "POINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE #+POSE\n",
    "\n",
    "NUM_NODES = len(POINT_LANDMARKS)\n",
    "CHANNELS = 6*NUM_NODES\n",
    "\n",
    "print(NUM_NODES)\n",
    "print(CHANNELS)\n",
    "\n",
    "def tf_nan_mean(x, axis=0, keepdims=False):\n",
    "    return tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims) / tf.reduce_sum(tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)), axis=axis, keepdims=keepdims)\n",
    "\n",
    "def tf_nan_std(x, center=None, axis=0, keepdims=False):\n",
    "    if center is None:\n",
    "        center = tf_nan_mean(x, axis=axis,  keepdims=True)\n",
    "    d = x - center\n",
    "    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n",
    "\n",
    "class Preprocess(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len=MAX_LEN, point_landmarks=POINT_LANDMARKS, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.point_landmarks = point_landmarks\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if tf.rank(inputs) == 3:\n",
    "            x = inputs[None,...]\n",
    "        else:\n",
    "            x = inputs\n",
    "        \n",
    "        mean = tf_nan_mean(tf.gather(x, [17], axis=2), axis=[1,2], keepdims=True)\n",
    "        mean = tf.where(tf.math.is_nan(mean), tf.constant(0.5,x.dtype), mean)\n",
    "        x = tf.gather(x, self.point_landmarks, axis=2) #N,T,P,C\n",
    "        std = tf_nan_std(x, center=mean, axis=[1,2], keepdims=True)\n",
    "        \n",
    "        x = (x - mean)/std\n",
    "\n",
    "        if self.max_len is not None:\n",
    "            x = x[:,:self.max_len]\n",
    "        length = tf.shape(x)[1]\n",
    "        x = x[...,:2]\n",
    "\n",
    "        dx = tf.cond(tf.shape(x)[1]>1,lambda:tf.pad(x[:,1:] - x[:,:-1], [[0,0],[0,1],[0,0],[0,0]], constant_values=1.),lambda:tf.ones_like(x))\n",
    "\n",
    "        dx2 = tf.cond(tf.shape(x)[1]>2,lambda:tf.pad(x[:,2:] - x[:,:-2], [[0,0],[0,2],[0,0],[0,0]], constant_values=1.),lambda:tf.ones_like(x))\n",
    "\n",
    "        x = tf.concat([\n",
    "            tf.reshape(x, (-1,length,2*len(self.point_landmarks))),\n",
    "            tf.reshape(dx, (-1,length,2*len(self.point_landmarks))),\n",
    "            tf.reshape(dx2, (-1,length,2*len(self.point_landmarks))),\n",
    "        ], axis = -1)\n",
    "        \n",
    "        x = tf.where(tf.math.is_nan(x),tf.constant(1.,x.dtype),x)#0.\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28a06bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:07:22.198680Z",
     "iopub.status.busy": "2023-06-02T01:07:22.198343Z",
     "iopub.status.idle": "2023-06-02T01:07:22.219910Z",
     "shell.execute_reply": "2023-06-02T01:07:22.218688Z"
    },
    "papermill": {
     "duration": 0.030592,
     "end_time": "2023-06-02T01:07:22.222291",
     "exception": false,
     "start_time": "2023-06-02T01:07:22.191699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ECA(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_size=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs, mask=mask)\n",
    "        nn = tf.expand_dims(nn, -1)\n",
    "        nn = self.conv(nn)\n",
    "        nn = tf.squeeze(nn, -1)\n",
    "        nn = tf.nn.sigmoid(nn)\n",
    "        nn = nn[:,None,:]\n",
    "        return inputs * nn\n",
    "\n",
    "class LateDropout(tf.keras.layers.Layer):\n",
    "    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.rate = rate\n",
    "        self.start_step = start_step\n",
    "        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
    "      \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # tf.cond(pred, true_fn=None, false_fn=None,) => return true_fn() if the predicate pred is true else false_fn().\n",
    "        # if self._train_counter and self.start_step are equal then false_fn is executed.\n",
    "        x = tf.cond(self._train_counter < self.start_step, lambda:inputs, lambda:self.dropout(inputs, training=training))\n",
    "        if training:\n",
    "            self._train_counter.assign_add(1)\n",
    "        return x\n",
    "\n",
    "class CausalDWConv1D(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "        kernel_size=17,\n",
    "        dilation_rate=1,\n",
    "        use_bias=False,\n",
    "        depthwise_initializer='glorot_uniform',\n",
    "        name='', **kwargs):\n",
    "        super().__init__(name=name,**kwargs)\n",
    "        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0),name=name + '_pad')\n",
    "        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n",
    "                            kernel_size,\n",
    "                            strides=1,\n",
    "                            dilation_rate=dilation_rate,\n",
    "                            padding='valid',\n",
    "                            use_bias=use_bias,\n",
    "                            depthwise_initializer=depthwise_initializer,\n",
    "                            name=name + '_dwconv')\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.causal_pad(inputs)\n",
    "        x = self.dw_conv(x)\n",
    "        return x\n",
    "\n",
    "def Conv1DBlock(channel_size,\n",
    "          kernel_size,\n",
    "          dilation_rate=1,\n",
    "          drop_rate=0.0,\n",
    "          expand_ratio=2,\n",
    "          se_ratio=0.25,\n",
    "          activation='swish',\n",
    "          name=None):\n",
    "    '''\n",
    "    efficient conv1d block, @hoyso48\n",
    "    '''\n",
    "    if name is None:\n",
    "        name = str(tf.keras.backend.get_uid(\"mbblock\"))\n",
    "    # Expansion phase\n",
    "    def apply(inputs):\n",
    "        channels_in = tf.keras.backend.int_shape(inputs)[-1]\n",
    "        channels_expand = channels_in * expand_ratio\n",
    "\n",
    "        skip = inputs\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channels_expand,\n",
    "            use_bias=True,\n",
    "            activation=activation,\n",
    "            name=name + '_expand_conv')(inputs)\n",
    "\n",
    "        # Depthwise Convolution\n",
    "        x = CausalDWConv1D(kernel_size,\n",
    "            dilation_rate=dilation_rate,\n",
    "            use_bias=False,\n",
    "            name=name + '_dwconv')(x)\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95, name=name + '_bn')(x)\n",
    "\n",
    "        x  = ECA()(x)\n",
    "\n",
    "        x = tf.keras.layers.Dense(\n",
    "            channel_size,\n",
    "            use_bias=True,\n",
    "            name=name + '_project_conv')(x)\n",
    "\n",
    "        if drop_rate > 0:\n",
    "            x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1), name=name + '_drop')(x)\n",
    "\n",
    "        if (channels_in == channel_size):\n",
    "            x = tf.keras.layers.add([x, skip], name=name + '_add')\n",
    "        return x\n",
    "\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3b8b3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:07:22.234482Z",
     "iopub.status.busy": "2023-06-02T01:07:22.233802Z",
     "iopub.status.idle": "2023-06-02T01:07:22.248919Z",
     "shell.execute_reply": "2023-06-02T01:07:22.247980Z"
    },
    "papermill": {
     "duration": 0.023793,
     "end_time": "2023-06-02T01:07:22.251249",
     "exception": false,
     "start_time": "2023-06-02T01:07:22.227456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim=256, num_heads=4, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.scale = self.dim ** -0.5\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = tf.keras.layers.Dense(3 * dim, use_bias=False)\n",
    "        self.drop1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.proj = tf.keras.layers.Dense(dim, use_bias=False)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        qkv = self.qkv(inputs)\n",
    "        qkv = tf.keras.layers.Permute((2, 1, 3))(tf.keras.layers.Reshape((-1, self.num_heads, self.dim * 3 // self.num_heads))(qkv))\n",
    "        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n",
    "\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask[:, None, None, :]\n",
    "\n",
    "        attn = tf.keras.layers.Softmax(axis=-1)(attn, mask=mask)\n",
    "        attn = self.drop1(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = tf.keras.layers.Reshape((-1, self.dim))(tf.keras.layers.Permute((2, 1, 3))(x))\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def TransformerBlock(dim=256, num_heads=4, expand=4, attn_dropout=0.2, drop_rate=0.2, activation='swish'):\n",
    "    def apply(inputs):\n",
    "        x = inputs\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
    "        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([inputs, x])\n",
    "        attn_out = x\n",
    "\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.95)(x)\n",
    "        x = tf.keras.layers.Dense(dim*expand, use_bias=False, activation=activation)(x)\n",
    "        x = tf.keras.layers.Dense(dim, use_bias=False)(x)\n",
    "        x = tf.keras.layers.Dropout(drop_rate, noise_shape=(None,1,1))(x)\n",
    "        x = tf.keras.layers.Add()([attn_out, x])\n",
    "        return x\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32cce51e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:07:22.263806Z",
     "iopub.status.busy": "2023-06-02T01:07:22.263487Z",
     "iopub.status.idle": "2023-06-02T01:07:22.274546Z",
     "shell.execute_reply": "2023-06-02T01:07:22.273559Z"
    },
    "papermill": {
     "duration": 0.019495,
     "end_time": "2023-06-02T01:07:22.276449",
     "exception": false,
     "start_time": "2023-06-02T01:07:22.256954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(max_len=MAX_LEN, dropout_step=0, dim=192):\n",
    "    inp = tf.keras.Input((max_len,CHANNELS))\n",
    "    #we don't need masking layer with inference.    \n",
    "    #x = tf.keras.layers.Masking(mask_value=PAD,input_shape=(max_len,CHANNELS))(inp) \n",
    "    x = inp\n",
    "    ksize = 17\n",
    "    x = tf.keras.layers.Dense(dim, use_bias=False,name='stem_conv')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.95,name='stem_bn')(x)\n",
    "\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "    x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "    if dim == 384: #for the 4x sized model\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = Conv1DBlock(dim,ksize,drop_rate=0.2)(x)\n",
    "        x = TransformerBlock(dim,expand=2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(dim*2,activation=None,name='top_conv')(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = LateDropout(0.8, start_step=dropout_step)(x)\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES,name='classifier')(x)\n",
    "    return tf.keras.Model(inp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1d9c7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:07:22.289169Z",
     "iopub.status.busy": "2023-06-02T01:07:22.287437Z",
     "iopub.status.idle": "2023-06-02T01:07:30.801279Z",
     "shell.execute_reply": "2023-06-02T01:07:30.800325Z"
    },
    "papermill": {
     "duration": 8.522441,
     "end_time": "2023-06-02T01:07:30.803757",
     "exception": false,
     "start_time": "2023-06-02T01:07:22.281316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select 3 folds if you want complete rid from \"submission scoring error\".\n",
    "models_path = [\n",
    "                '/kaggle/input/islr-weights/weights/islr-fp16-192-8-seed42-fold0-best.h5', \n",
    "#                 '/kaggle/input/islr-weights/weights/islr-fp16-192-8-seed42-fold1-best.h5',\n",
    "                '/kaggle/input/islr-weights/weights/islr-fp16-192-8-seed42-fold2-best.h5',\n",
    "                '/kaggle/input/islr-weights/weights/islr-fp16-192-8-seed42-fold3-best.h5',\n",
    "                '/kaggle/input/islr-weights/weights/islr-fp16-192-8-seed42-fold4-best.h5',    \n",
    "              ]\n",
    "models = [get_model() for _ in models_path]\n",
    "for model,path in zip(models,models_path):\n",
    "    model.load_weights(path)\n",
    "# models[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b9df62c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:07:30.816683Z",
     "iopub.status.busy": "2023-06-02T01:07:30.815032Z",
     "iopub.status.idle": "2023-06-02T01:07:30.823970Z",
     "shell.execute_reply": "2023-06-02T01:07:30.823095Z"
    },
    "papermill": {
     "duration": 0.01701,
     "end_time": "2023-06-02T01:07:30.826045",
     "exception": false,
     "start_time": "2023-06-02T01:07:30.809035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TFLiteModel(tf.Module):\n",
    "    \"\"\"\n",
    "    TensorFlow Lite model that takes input tensors and applies:\n",
    "        – a preprocessing model\n",
    "        – the ISLR model \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, islr_models):\n",
    "        \"\"\"\n",
    "        Initializes the TFLiteModel with the specified preprocessing model and ISLR model.\n",
    "        \"\"\"\n",
    "        super(TFLiteModel, self).__init__()\n",
    "\n",
    "        # Load the feature generation and main models\n",
    "        self.prep_inputs = Preprocess()\n",
    "        self.islr_models   = islr_models\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, 543, 3], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Applies the feature generation model and main model to the input tensors.\n",
    "        Args:\n",
    "            inputs: Input tensor with shape [batch_size, 543, 3].\n",
    "        Returns:\n",
    "            A dictionary with a single key 'outputs' and corresponding output tensor.\n",
    "        \"\"\"\n",
    "        x = self.prep_inputs(tf.cast(inputs, dtype=tf.float32))\n",
    "        outputs = [model(x) for model in self.islr_models]\n",
    "        \n",
    "        # tf.keras.layers.Average => layer that averages a list of inputs element-wise.\n",
    "        outputs = tf.keras.layers.Average()(outputs)[0]\n",
    "        return {'outputs': outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63f033a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:07:30.836671Z",
     "iopub.status.busy": "2023-06-02T01:07:30.836383Z",
     "iopub.status.idle": "2023-06-02T01:07:40.997397Z",
     "shell.execute_reply": "2023-06-02T01:07:40.996432Z"
    },
    "papermill": {
     "duration": 10.168884,
     "end_time": "2023-06-02T01:07:40.999679",
     "exception": false,
     "start_time": "2023-06-02T01:07:30.830795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blow'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_keras_model = TFLiteModel(islr_models=models)\n",
    "\n",
    "\n",
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet('/kaggle/input/asl-signs/' + pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "demo_output = tflite_keras_model(load_relevant_data_subset(train_df.path[0]))[\"outputs\"]\n",
    "decoder(np.argmax(demo_output.numpy(), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a13243c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:07:41.012868Z",
     "iopub.status.busy": "2023-06-02T01:07:41.011249Z",
     "iopub.status.idle": "2023-06-02T01:09:24.174426Z",
     "shell.execute_reply": "2023-06-02T01:09:24.172872Z"
    },
    "papermill": {
     "duration": 103.172965,
     "end_time": "2023-06-02T01:09:24.178050",
     "exception": false,
     "start_time": "2023-06-02T01:07:41.005085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/model.tflite (deflated 9%)\r\n"
     ]
    }
   ],
   "source": [
    "# tf.lite.TFLiteConverter.from_keras_model(...).convert() => converts a TensorFlow model into TensorFlow Lite model.\n",
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflite_keras_model)\n",
    "# tf.lite.Optimize => defining the optimizations to apply when generating a tflite model.\n",
    "# .DEFAULT => enables post-training quantization that can reduce model size while also improving CPU and hardware accelerator latency, with little degradation in model accuracy.\n",
    "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "keras_model_converter.target_spec.supported_types = [tf.float16]\n",
    "# .convert() => converts a TensorFlow GraphDef based on instance variables.\n",
    "tflite_model = keras_model_converter.convert()\n",
    "with open('/kaggle/working/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "!zip submission.zip /kaggle/working/model.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10fc2177",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-02T01:09:24.205984Z",
     "iopub.status.busy": "2023-06-02T01:09:24.204971Z",
     "iopub.status.idle": "2023-06-02T01:09:24.241890Z",
     "shell.execute_reply": "2023-06-02T01:09:24.240968Z"
    },
    "papermill": {
     "duration": 0.053152,
     "end_time": "2023-06-02T01:09:24.244048",
     "exception": false,
     "start_time": "2023-06-02T01:09:24.190896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check inference time\n",
    "# code from @hengck23 =>\n",
    "# https://www.kaggle.com/code/hengck23/lb-0-67-one-pytorch-transformer-solution?scriptVersionId=122239639\n",
    "mode = 's' #'d'ebug #'s'ubmit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "if mode in ['d']:  \n",
    "    try:\n",
    "        import tflite_runtime\n",
    "    except:\n",
    "        # tflite-runtime => for running machine learning models on mobile and embedded devices. \n",
    "        !pip3 install tflite-runtime\n",
    "\n",
    "    import tflite_runtime.interpreter as tflite   \n",
    "    import tflite_runtime\n",
    "    print(tflite_runtime.__version__)\n",
    "\n",
    "\n",
    "'''\n",
    "Your model must also require less than 40 MB in memory and \n",
    "perform inference with less than 100 milliseconds of latency per video. \n",
    "Expect to see approximately 40,000 videos in the test set. \n",
    "We allow an additional 10 minute buffer for loading the data and miscellaneous overhead.\n",
    "'''\n",
    "def time_to_str(t, mode='min'):\n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "ROWS_PER_FRAME = 543\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "if mode in ['d']: \n",
    " \n",
    "    interpreter = tflite.Interpreter('/kaggle/working/model.tflite')\n",
    "    prediction_fn = interpreter.get_signature_runner('serving_default')\n",
    "#     valid_df = pd.read_csv('/kaggle/input/asl-demo/train_prepared.csv') \n",
    "#     valid_df = train_df[train_df.fold==0].reset_index(drop=True)\n",
    "#     valid_df = valid_df[:1000]\n",
    "    valid_df = train_df[:1000]\n",
    "    valid_num = len(valid_df)\n",
    "    valid = {\n",
    "        'sign':[],\n",
    "    }\n",
    "\n",
    "    start_timer = timer()\n",
    "    for t, d in valid_df.iterrows():\n",
    "\n",
    "        pq_file = f'/kaggle/input/asl-signs/{d.path}'\n",
    "        #print(pq_file)\n",
    "        xyz = load_relevant_data_subset(pq_file)\n",
    "\n",
    "        output = prediction_fn(inputs=xyz)\n",
    "        p = output['outputs'].reshape(-1)\n",
    "\n",
    "        valid['sign'].append(p)\n",
    "\n",
    "        #---\n",
    "        if t%100==0:\n",
    "            time_taken = timer() - start_timer\n",
    "            print('\\r %8d / %d  %s'%(t,valid_num,time_to_str(time_taken,'sec')),end='',flush=True)\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "    truth = valid_df.label.values\n",
    "    sign  = np.stack(valid['sign'])\n",
    "    predict = np.argsort(-sign, -1)\n",
    "    correct = predict==truth.reshape(valid_num,1)\n",
    "    topk = correct.cumsum(-1).mean(0)[:5]\n",
    "\n",
    "\n",
    "    print(f'time_taken = {time_to_str(time_taken,\"sec\")}')\n",
    "    print(f'time_taken for LB = {time_taken*1000/valid_num:05f} msec\\n')\n",
    "    for i in range(5):\n",
    "        print(f'topk[{i}] = {topk[i]}')  \n",
    "    print('----- end -----\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99758aaf",
   "metadata": {
    "papermill": {
     "duration": 0.008098,
     "end_time": "2023-06-02T01:09:24.260171",
     "exception": false,
     "start_time": "2023-06-02T01:09:24.252073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 144.097422,
   "end_time": "2023-06-02T01:09:27.191551",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-02T01:07:03.094129",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
